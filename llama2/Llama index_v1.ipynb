{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bd62fae-10ff-455e-82f9-6007244ff0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - You need to install both C++ Desktop development from windows and Windows 10 SDK \n",
    "#!pip install llama-index\n",
    "#!pip install llama-index-embeddings-huggingface\n",
    "#!pip install llama-index-llms-llama-cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0cbe9c5-cce5-42bf-9030-1b7acf88f87a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\llm\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "from llama_index.llms.llama_cpp.llama_utils import (\n",
    "    messages_to_prompt,\n",
    "    completion_to_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "575262ba-6250-4f37-8ed5-6a6dcf9ac17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from models/mistral-7b-instruct-v0.2.Q2_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 10\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q2_K:   65 tensors\n",
      "llama_model_loader: - type q3_K:  160 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q2_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 2.87 GiB (3.41 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  2939.57 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 3900\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   487.50 MiB\n",
      "llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    16.65 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   275.75 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '10', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '1000000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\"}\n",
      "Guessed chat format: mistral-instruct\n"
     ]
    }
   ],
   "source": [
    "llm = LlamaCPP(\n",
    "    # You can pass in the URL to a GGML model to download it automatically\n",
    "   \n",
    "    # optionally, you can set the path to a pre-downloaded model instead of model_url\n",
    "    model_path=\"models/mistral-7b-instruct-v0.2.Q2_K.gguf\",\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=256,\n",
    "    # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\n",
    "    context_window=3900,\n",
    "    # kwargs to pass to __call__()\n",
    "    generate_kwargs={},\n",
    "    # kwargs to pass to __init__()\n",
    "    # set to at least 1 to use GPU\n",
    "    model_kwargs={\"n_gpu_layers\": 0},\n",
    "    messages_to_prompt=messages_to_prompt,\n",
    "    completion_to_prompt=completion_to_prompt,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86536a20-443d-41f3-8ef0-b84f98166145",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   10909.03 ms\n",
      "llama_print_timings:      sample time =      46.11 ms /   195 runs   (    0.24 ms per token,  4229.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10908.80 ms /    76 tokens (  143.54 ms per token,     6.97 tokens per second)\n",
      "llama_print_timings:        eval time =   47497.83 ms /   194 runs   (  244.83 ms per token,     4.08 tokens per second)\n",
      "llama_print_timings:       total time =   58960.88 ms /   270 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Certainly! Here's a classic poem that compares cats and dogs:\n",
      "\n",
      "In a house of my own, there's a cat and there's a dog,\n",
      "Each one with a different way to beg for a bog.\n",
      "The cat with his paw, and the dog with his eyes,\n",
      "They both have their ways to ensnare the cries.\n",
      "\n",
      "The cat with his grace, and the dog with his might,\n",
      "They share the same home in the soft, quiet night.\n",
      "The cat with his purr, and the dog with his bark,\n",
      "They each have their charms to leave their mark.\n",
      "\n",
      "The cat with his secrets, and the dog with his loyalty,\n",
      "They both have their stories to tell, in their own way,\n",
      "In the house of my own, where the cat and the dog play,\n",
      "Their bond is a testament to the joy of each day.\n"
     ]
    }
   ],
   "source": [
    "response = llm.complete(\"Hello! Can you tell me a poem about cats and dogs?\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8cc4423-3b5f-4849-afce-d26f8cf8062f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " In the realm where metal beasts come to play,\n",
      "Where engines roar and rubber hits the way,\n",
      "Lies the thrill of speed, a captivating display,\n",
      "Of fast cars, in their grand and glorious array.\n",
      "\n",
      "Engines purring, revving with might,\n",
      "Beneath the sun's resplendent light,\n",
      "They race against the clock, day and night,\n",
      "In the dance of power, an exhilarating sight.\n",
      "\n",
      "Sleek bodies glisten in the morning dew,\n",
      "Reflections of the world, a breathtaking view,\n",
      "Through the windshield, life's complexities subdue,\n",
      "As the fast cars weave through the landscape, ever new.\n",
      "\n",
      "Their roar, a testament to human might,\n",
      "A symphony of sound, an unforgettable sight,\n",
      "In the realm of fast cars, we find our light,\n",
      "A thrilling escape from the mundane, day or night."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   10909.03 ms\n",
      "llama_print_timings:      sample time =      50.09 ms /   212 runs   (    0.24 ms per token,  4232.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2058.85 ms /    14 tokens (  147.06 ms per token,     6.80 tokens per second)\n",
      "llama_print_timings:        eval time =   51999.47 ms /   211 runs   (  246.44 ms per token,     4.06 tokens per second)\n",
      "llama_print_timings:       total time =   54806.53 ms /   225 tokens\n"
     ]
    }
   ],
   "source": [
    "response_iter = llm.stream_complete(\"Can you write me a poem about fast cars?\")\n",
    "for response in response_iter:\n",
    "    print(response.delta, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac22b431-1891-4e2f-b6d1-8122432522a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Nuclear fission is a process where the nucleus of an atom is split into two or more smaller nuclei, releasing energy in the form of heat and radiation. This process is initiated most commonly by neutron irradiation of an atomic nucleus, which can cause the nucleus to become unstable and split.\n",
      "\n",
      "The most common type of nuclear fission is that of uranium-235 (U-235). When a neutron collides with the U-235 nucleus, it causes the nucleus to become unstable and undergo fission. The U-235 nucleus can split into two smaller nuclei, such as barium-141 and krypton-92, along with some neutrons and gamma radiation. The total mass of the resulting nuclei is slightly less than that of the original U-235 nucleus, resulting in the release of energy in the form of heat.\n",
      "\n",
      "The released neutrons can then go on to cause further fissions in other U-235 nuclei, leading to a chain reaction. This is the fundamental principle behind the operation of nuclear power plants and atomic bombs.\n",
      "\n",
      "The energy released from nuclear"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   15865.81 ms\n",
      "llama_print_timings:      sample time =      58.48 ms /   256 runs   (    0.23 ms per token,  4377.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15865.66 ms /    71 tokens (  223.46 ms per token,     4.48 tokens per second)\n",
      "llama_print_timings:        eval time =   60267.06 ms /   255 runs   (  236.34 ms per token,     4.23 tokens per second)\n",
      "llama_print_timings:       total time =   77031.65 ms /   326 tokens\n"
     ]
    }
   ],
   "source": [
    "response_iter = llm.stream_complete(\"Explain nuclear fission in detail?\")\n",
    "for response in response_iter:\n",
    "    print(response.delta, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94c646e0-6ed1-48d1-a619-52695f6c448d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Based on the information provided, you initially had 10 apples, you ate 13 apples (10 apples that you had + 3 apples that were not mentioned but can be inferred were eaten), and you stole 2 apples. Therefore, you currently have 12 apples (10 apples that you initially had + 2 apples that you stole)."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   15865.81 ms\n",
      "llama_print_timings:      sample time =      18.02 ms /    84 runs   (    0.21 ms per token,  4662.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1500.48 ms /    13 tokens (  115.42 ms per token,     8.66 tokens per second)\n",
      "llama_print_timings:        eval time =   19015.41 ms /    83 runs   (  229.10 ms per token,     4.36 tokens per second)\n",
      "llama_print_timings:       total time =   20788.91 ms /    96 tokens\n"
     ]
    }
   ],
   "source": [
    "response_iter = llm.stream_complete(\"I have 10 apples and 2 oranges, I ate 13 apples and 1 orange. I stole 2 apples from my friend. How many apples do I have? - Note this might be a trick questin\")\n",
    "for response in response_iter:\n",
    "    print(response.delta, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dfa807-6efb-43a1-bf6f-11b72bc96583",
   "metadata": {},
   "source": [
    "<h1> Query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5131635-9bd4-439a-be28-5b3e1af4a54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\llm\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import set_global_tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "set_global_tokenizer(\n",
    "    AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\").encode\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0032019-3cd5-4e2b-a647-e4650244a2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\llm\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\AppData\\Local\\llama_index\\models--BAAI--bge-small-en-v1.5. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "# use Huggingface embeddings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17052dd3-0b1f-4582-ac1e-b8043c54305a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load documents\n",
    "documents = SimpleDirectoryReader(\n",
    "    \"./data\"\n",
    ").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7cfc0e8f-8495-47a0-9e27-1a0ce61b53b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vector store index\n",
    "index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f70b9fd-3ad3-4a8e-a255-fb02d322cbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up query engine\n",
    "query_engine = index.as_query_engine(llm=llm,streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c5c013bf-fe2c-4c39-9e57-f79aa5a982d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_response = query_engine.query(\n",
    "    \"What did the author do growing up?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "890ecb77-e8c3-483a-bf4c-ca45e54914f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " author grew up working on writing and programming outside of school. They wrote short stories with little plot and tried programming on an IBM 1401 computer in 9th grade, but couldn't figure out what to do with it due to the lack of input options. They were impressed by the microcomputers that came later, which allowed for real-time interaction, and started programming on a TRS-80 microcomputer in about 1980. Despite their interest in programming, they didn't plan to study it in college and instead pursued philosophy. However, they found philosophy courses to be boring and eventually switched to study Artificial Intelligence (AI) instead. They also discovered a passion for painting and started taking art classes at Harvard while still in a PhD program in computer science. They applied to art schools and eventually attended Rhode Island School of Design (RISD) for a BFA program."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   16329.43 ms\n",
      "llama_print_timings:      sample time =      41.75 ms /   191 runs   (    0.22 ms per token,  4574.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =  122394.67 ms /   912 tokens (  134.20 ms per token,     7.45 tokens per second)\n",
      "llama_print_timings:        eval time =   48871.36 ms /   190 runs   (  257.22 ms per token,     3.89 tokens per second)\n",
      "llama_print_timings:       total time =  171973.91 ms /  1102 tokens\n"
     ]
    }
   ],
   "source": [
    "for text in streaming_response.response_gen:\n",
    "    streaming_response.print_response_stream()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aeb663d0-d2a4-478a-a76d-4a335e6a452e",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_response = query_engine.query(\n",
    "    \"What is this story about?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "18393076-695a-40da-aff7-4a08c3b6ba7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " story is about the author's experiences with programming and writing before and during college. The author shares their early attempts at writing short stories and programming on an IBM 1401 computer in junior high school. They also discuss their later experiences with microcomputers and their fascination with the ability to write programs directly into the computer. The author also shares their college experiences, where they studied philosophy but found it to be uninteresting and eventually switched to Artificial Intelligence (AI)."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   16329.43 ms\n",
      "llama_print_timings:      sample time =      21.95 ms /   100 runs   (    0.22 ms per token,  4556.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =  204567.49 ms /  1534 tokens (  133.36 ms per token,     7.50 tokens per second)\n",
      "llama_print_timings:        eval time =   25646.00 ms /    99 runs   (  259.05 ms per token,     3.86 tokens per second)\n",
      "llama_print_timings:       total time =  230567.40 ms /  1633 tokens\n"
     ]
    }
   ],
   "source": [
    "for text in streaming_response.response_gen:\n",
    "    streaming_response.print_response_stream()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d296332-4965-4e42-8699-c8a8477b44c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " author's first experiences with programming were on an IBM 1401 computer in junior high school, where they were unable to write programs that relied on input from punched cards due to lacking data stored on such cards. They were also unable to write programs that didn't require input, as they didn't have enough mathematical knowledge to calculate interesting things. Their first clear memory of programming was learning that programs could run indefinitely without terminating, causing an issue for the data center manager. With microcomputers, they were able to write programs in real-time and started programming seriously, but they initially planned to study philosophy in college until they discovered that the field was mostly dealing with edge cases that other fields had ignored."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   16329.43 ms\n",
      "llama_print_timings:      sample time =      34.58 ms /   152 runs   (    0.23 ms per token,  4395.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =   79551.71 ms /   590 tokens (  134.83 ms per token,     7.42 tokens per second)\n",
      "llama_print_timings:        eval time =   38146.37 ms /   151 runs   (  252.62 ms per token,     3.96 tokens per second)\n",
      "llama_print_timings:       total time =  118224.92 ms /   741 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "streaming_response = query_engine.query(\n",
    "    \"tell me an interesting fact about in the story?\",\n",
    ")\n",
    "for text in streaming_response.response_gen:\n",
    "    streaming_response.print_response_stream()\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f5b8fafe-9c84-4b49-8fa8-9e1062fec965",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " author does not provide any specific information about his parents or siblings in the provided context. As for food preferences, there is no mention of them in the text."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   16329.43 ms\n",
      "llama_print_timings:      sample time =       7.24 ms /    34 runs   (    0.21 ms per token,  4695.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =  248260.23 ms /  1900 tokens (  130.66 ms per token,     7.65 tokens per second)\n",
      "llama_print_timings:        eval time =    8522.72 ms /    33 runs   (  258.26 ms per token,     3.87 tokens per second)\n",
      "llama_print_timings:       total time =  256904.83 ms /  1933 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "streaming_response = query_engine.query(\n",
    "    \"What is the author's family - tell me about his parents, siblings if any? Also tell me his food preferences\",\n",
    ")\n",
    "for text in streaming_response.response_gen:\n",
    "    streaming_response.print_response_stream()\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "32410ec1-3685-4ca3-b64e-3c86d13bb8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " individual in the text describes his experiences with programming starting from the IBM 1401 computer in the late 1960s using an early version of Fortran, to microcomputers like the TRS-80 in the late 1970s and early 1980s when he wrote simple games and a word processor using simple BASIC. In college, he was more interested in philosophy but switched to Artificial Intelligence (AI) due to boredom with philosophy courses. He mentions McCarthy's Lisp but does not create or code it himself. Instead, he attempts to create a new Lisp interpreter called Bel from March 2015 to October 2019 in Arc using a collection of hacks to make it work. He writes essays during his work on Bel but stops writing them to focus on Bel's development. He moves to England in 2016 with his family and finishes Bel in the fall of 2019. Bel is a spec rather than an implementation expressed as code. The individual continues writing essays after Bel's completion. Therefore, the programming languages he mentions in the text are Fortran, BASIC, and Lisp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   16329.43 ms\n",
      "llama_print_timings:      sample time =      55.32 ms /   256 runs   (    0.22 ms per token,  4627.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =  247702.42 ms /  1891 tokens (  130.99 ms per token,     7.63 tokens per second)\n",
      "llama_print_timings:        eval time =   66465.54 ms /   255 runs   (  260.65 ms per token,     3.84 tokens per second)\n",
      "llama_print_timings:       total time =  315121.41 ms /  2146 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "streaming_response = query_engine.query(\n",
    "    \"Where did he work, what all programmming languages did he create or code?\",\n",
    ")\n",
    "for text in streaming_response.response_gen:\n",
    "    streaming_response.print_response_stream()\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "48b8ffe9-e072-4256-9b58-3d2edf524bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " document mentions the following people:\n",
      "\n",
      "1. John McCarthy\n",
      "2. Rich Draves\n",
      "3. The data center manager\n",
      "4. My friend who built his own microcomputer (name unmentioned)\n",
      "5. My father\n",
      "6. Paul Graham (mentioned in the present tense in the author's reflection)\n",
      "7. The readers of HN (mentioned in the past tense)\n",
      "8. The boys (mentioned in the present tense)\n",
      "9. The data processing staff at the junior high school (implied)\n",
      "10. The designers of Fortran (implied)\n",
      "11. The designers of the IBM 1401 (implied)\n",
      "12. The designers of the TRS-80 and Apple II (implied)\n",
      "13. The designers of other computers (implied)\n",
      "14. The authors of the other fields of study (implied)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   16329.43 ms\n",
      "llama_print_timings:      sample time =      44.49 ms /   197 runs   (    0.23 ms per token,  4427.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =  253107.58 ms /  1887 tokens (  134.13 ms per token,     7.46 tokens per second)\n",
      "llama_print_timings:        eval time =   53417.42 ms /   196 runs   (  272.54 ms per token,     3.67 tokens per second)\n",
      "llama_print_timings:       total time =  307262.96 ms /  2083 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "streaming_response = query_engine.query(\n",
    "    \"Give me the list of people he has mentioned in the document?\",\n",
    ")\n",
    "for text in streaming_response.response_gen:\n",
    "    streaming_response.print_response_stream()\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
