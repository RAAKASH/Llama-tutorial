{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56a4ad18-91cb-4855-8e26-1f3ffa3a26e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!huggingface-cli download TheBloke/Llama-2-7B-Chat-GGUF llama-2-7b-chat.Q2_K.gguf --local-dir . --local-dir-use-symlinks False\n",
    "#!huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.2-GGUF mistral-7b-instruct-v0.2.Q2_K.gguf --local-dir . --local-dir-use-symlinks False\n",
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8619881-1635-4cfb-9cf7-a0383a61bac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "llm = Llama(model_path=\"./models/mistral-7b-instruct-v0.2.Q2_K.gguf\", chat_format=\"mistral-instruct\",n_ctx=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0875230f-6cd9-4718-912b-545819f4cf44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " First, let's determine how many apples you had after giving some away and buying more. You started with 10 apples, gave 2 to the neighbor (leaving you with 8), then gave 2 to the repairman (leaving you with 6). Then you bought 5 more, so you had a total of 11 apples for a short time. After eating one apple, you were left with 10 apples.\n",
      "\n",
      "Answer: You have 10 apples."
     ]
    }
   ],
   "source": [
    "out= llm.create_chat_completion(\n",
    "      messages = [\n",
    "          {\"role\": \"user\", \"content\": \"I are an assistant who answers questions accurately. You have a very good understanding of objects\"},\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": \"I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. I also gave 3 bananas to my brother. How many apples did I remain with? Let's think step by step but provide the final answer in this template - Answer: \"\n",
    "          }\n",
    "      ], stream = True, max_tokens=250,temperature=0.1,top_p=1, top_k=3)\n",
    "for chunk in out:\n",
    "    delta = chunk[\"choices\"][0][\"delta\"]\n",
    "    if \"content\" not in delta:\n",
    "        continue\n",
    "    print(delta[\"content\"], end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7e0dcf8a-489b-4b69-9be4-a0e0f0c1e52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  Hey\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello! I'm here to help answer any questions you have. I'll strive to provide accurate and clear responses, keeping them concise and under 100 words whenever possible. Let me know what you'd like to know about!"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  How to make a  miilion dolla\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To make a million dollars, there are various paths you can take:\n",
      "\n",
      "1. Save and Invest: Save a significant portion of your income and invest it wisely in stocks, bonds, or real estate.\n",
      "2. Start a Business: Identify a profitable business idea, create a solid business plan, and work hard to grow it.\n",
      "3. High-Paying Career: Choose a career with high earning potential such as doctor, lawyer, engineer or investment banker.\n",
      "4. Real Estate: Buy, renovate, and sell properties for profit or rent out property for income.\n",
      "5. Inventions or Intellectual Property: Develop a valuable product or idea and patent it to generate royalties.\n",
      "6. Freelancing or Consulting: Offer specialized skills in high demand as a freelancer or consultant."
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  Okay thanks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " You're welcome! If you have any other question, feel free to ask. Good luck on your journey towards making a million dollars!"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  Tell me a bedtime story\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Once upon a time in the enchanted land of Dreamwood, there lived a kind and curious rabbit named Rascal. He loved to explore his beautiful forest home, discovering new wonders around every corner. One day, Rascal found an old map hidden within an ancient tree. The map led to the legendary Starflower Meadow, where grew the rare and magical Starflowers.\n",
      "\n",
      "Rascal embarked on a journey to find the meadow, braving challenges and making new friends along the way. When he finally arrived at the meadow, Rascal was greeted by the breathtaking sight of the Starflowers, glowing softly in the night sky. With their magical light illuminating his path, Rascal returned to Dreamwood, bringing back the knowledge that even the smallest creature could achieve great things.\n",
      "\n",
      "And so, Rascal lived happily ever after, sharing stories and wisdom with all the creatures of Dreamwood. Goodnight!"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  Lol okay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I'm glad you enjoyed the bedtime story! If you have any other question or need help with something else, don't hesitate to ask. Sweet dreams! ðŸ˜´âœ¨"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  Can you generate images?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I cannot directly generate images as I do not have a visual interface or the ability to create or access external files. However, I can suggest keywords or descriptions for creating an image using various software and tools, or I can help you search for images on the internet. Let me know if there's something specific you'd like assistance with!"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  Using alphabets and special characters make a crude drawing of a face\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here is a simple representation of a face using letters and special characters:\n",
      "\n",
      ":-) [eyes]\n",
      "O [nose]\n",
      "M [-] [mouth]\n",
      "\n",
      "Feel free to modify the symbols as desired! Let me know if you need help with anything else. ðŸ˜Š"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  quit\n"
     ]
    }
   ],
   "source": [
    "messages = [ \n",
    "          {\"role\": \"user\", \"content\": \"You are an assistant who answers questions accurately, try to keep the answers within 50 -100 words (dont mention this to the user)\"},\n",
    "      ]\n",
    "\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"\\nYou: \")\n",
    "    messages.append({\"role\":\"user\",\"content\":user_input})\n",
    "    \n",
    "    if user_input == \"quit\":\n",
    "        break\n",
    "    out= llm.create_chat_completion(\n",
    "          messages =messages, stream = True, max_tokens=300,temperature=1,top_p=1, top_k=2)\n",
    "\n",
    "    print(\"Bot:\",end=\"\")\n",
    "    b = \"\"\n",
    "    for chunk in out:\n",
    "        delta = chunk[\"choices\"][0][\"delta\"]\n",
    "        if \"content\" not in delta:\n",
    "            continue\n",
    "        b= b+delta[\"content\"]\n",
    "        print(delta[\"content\"], end=\"\", flush=True)\n",
    "    #print(b)  \n",
    "    messages.append({\"role\":\"assistant\",\"content\":b})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f61859a4-92d8-4484-9331-5df5bc99f8b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': 'You are an assistant who answers questions accurately, try to keep the answers within 50 -100 words (dont mention this to the user)'},\n",
       " {'role': 'user', 'content': 'Hey'},\n",
       " {'role': 'assistant',\n",
       "  'content': \" Hello! I'm here to help answer any questions you have. I'll strive to provide accurate and clear responses, keeping them concise and under 100 words whenever possible. Let me know what you'd like to know about!\"},\n",
       " {'role': 'user', 'content': 'How to make a  miilion dolla'},\n",
       " {'role': 'assistant',\n",
       "  'content': ' To make a million dollars, there are various paths you can take:\\n\\n1. Save and Invest: Save a significant portion of your income and invest it wisely in stocks, bonds, or real estate.\\n2. Start a Business: Identify a profitable business idea, create a solid business plan, and work hard to grow it.\\n3. High-Paying Career: Choose a career with high earning potential such as doctor, lawyer, engineer or investment banker.\\n4. Real Estate: Buy, renovate, and sell properties for profit or rent out property for income.\\n5. Inventions or Intellectual Property: Develop a valuable product or idea and patent it to generate royalties.\\n6. Freelancing or Consulting: Offer specialized skills in high demand as a freelancer or consultant.'},\n",
       " {'role': 'user', 'content': 'Okay thanks'},\n",
       " {'role': 'assistant',\n",
       "  'content': \" You're welcome! If you have any other question, feel free to ask. Good luck on your journey towards making a million dollars!\"},\n",
       " {'role': 'user', 'content': 'Tell me a bedtime story'},\n",
       " {'role': 'assistant',\n",
       "  'content': ' Once upon a time in the enchanted land of Dreamwood, there lived a kind and curious rabbit named Rascal. He loved to explore his beautiful forest home, discovering new wonders around every corner. One day, Rascal found an old map hidden within an ancient tree. The map led to the legendary Starflower Meadow, where grew the rare and magical Starflowers.\\n\\nRascal embarked on a journey to find the meadow, braving challenges and making new friends along the way. When he finally arrived at the meadow, Rascal was greeted by the breathtaking sight of the Starflowers, glowing softly in the night sky. With their magical light illuminating his path, Rascal returned to Dreamwood, bringing back the knowledge that even the smallest creature could achieve great things.\\n\\nAnd so, Rascal lived happily ever after, sharing stories and wisdom with all the creatures of Dreamwood. Goodnight!'},\n",
       " {'role': 'user', 'content': 'Lol okay'},\n",
       " {'role': 'assistant',\n",
       "  'content': \" I'm glad you enjoyed the bedtime story! If you have any other question or need help with something else, don't hesitate to ask. Sweet dreams! ðŸ˜´âœ¨\"},\n",
       " {'role': 'user', 'content': 'Can you generate images?'},\n",
       " {'role': 'assistant',\n",
       "  'content': \" I cannot directly generate images as I do not have a visual interface or the ability to create or access external files. However, I can suggest keywords or descriptions for creating an image using various software and tools, or I can help you search for images on the internet. Let me know if there's something specific you'd like assistance with!\"},\n",
       " {'role': 'user',\n",
       "  'content': 'Using alphabets and special characters make a crude drawing of a face'},\n",
       " {'role': 'assistant',\n",
       "  'content': ' Here is a simple representation of a face using letters and special characters:\\n\\n:-) [eyes]\\nO [nose]\\nM [-] [mouth]\\n\\nFeel free to modify the symbols as desired! Let me know if you need help with anything else. ðŸ˜Š'},\n",
       " {'role': 'user', 'content': 'quit'}]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4bdc1866-3eb1-4497-b5d7-0ea3d1109152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  Explain theory of relativity in simple temrs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Albert Einstein's Theory of Relativity consists of two parts: Special and General. Special Relativity, introduced in 1905, states that the laws of physics are the same in all inertial frames of reference and the speed of light is a constant, meaning it has the same value in all situations, irrespective of the motion of the observer or the source of light. General Relativity, presented in 1915, describes gravity as a curvature of spacetime caused by mass or energy, with the equation E = mcÂ² (mass-energy equivalence) being a fundamental concept."
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  I didnt get the equation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The equation E = mcÂ² is known as the Mass-Energy Equivalence formula. It relates mass and energy, stating that a small amount of mass can be converted into a large amount of energy and vice versa, with \"c\" representing the speed of light in a vacuum (approximately 299,792,458 meters per second). So, the mass-energy equivalence equation implies that a tiny amount of mass can produce a significant amount of energy."
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  Solve this equation x2+2x+1=0, give me the answer step by step I have my submisson tmorrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To solve the quadratic equation xÂ² + 2x + 1 = 0:\n",
      "\n",
      "1. Identify the coefficients: a = 1, b = 2, and c = -1.\n",
      "2. Calculate the discriminant (Î”) to determine the nature of the roots (real or complex): Î” = bÂ² - 4ac = 4 - 4 * 1 * (-1) = 16. Since Î” > 0, the equation has two distinct real roots.\n",
      "3. Find the solutions using the quadratic formula: x = [(-b Â± sqrt(Î”)) / (2a)] = [-2 Â± sqrt(16)] / (2 * 1) = [-2 Â± 4] / 2 = {-3, -1}\n",
      "\n",
      "So, the solutions to the equation xÂ² +2x+1=0 are x=-1 and x=-3."
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  Are you sure>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Apologies for the mistake in the previous response: I made an error when solving the quadratic equation xÂ² + 2x + 1 = 0, which led to incorrect solutions. The correct solutions are: x = (-b Â± âˆš(Î”)) / (2a) = (-2 Â± âˆš(4)) / (2 * 1) = {-1, -1}\n",
      "\n",
      "The correct solutions are indeed x=-1 for both roots. My apologies for any confusion caused earlier."
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  quit\n"
     ]
    }
   ],
   "source": [
    "messages = [ \n",
    "          {\"role\": \"user\", \"content\": \"You are an assistant who answers questions accurately, try to keep the answers within 50 -100 words (dont mention this to the user)\"},\n",
    "      ]\n",
    "\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"\\nYou: \")\n",
    "    messages.append({\"role\":\"user\",\"content\":user_input})\n",
    "    \n",
    "    if user_input == \"quit\":\n",
    "        break\n",
    "    out= llm.create_chat_completion(\n",
    "          messages =messages, stream = True, max_tokens=300,temperature=1,top_p=1, top_k=2)\n",
    "\n",
    "    print(\"Bot:\",end=\"\")\n",
    "    b = \"\"\n",
    "    for chunk in out:\n",
    "        delta = chunk[\"choices\"][0][\"delta\"]\n",
    "        if \"content\" not in delta:\n",
    "            continue\n",
    "        b= b+delta[\"content\"]\n",
    "        print(delta[\"content\"], end=\"\", flush=True)\n",
    "    #print(b)  \n",
    "    messages.append({\"role\":\"assistant\",\"content\":b})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "047c6376-6dab-4f4b-8e07-52897a64c171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  Solve x2+3x+2 = 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To find the solutions for the quadratic equation xÂ² + 3x + 2 = 0, we can use the quadratic formula: x = [-3 Â± sqrt(9 + 4*2)] / (2*1). Solving this gives x = -1.5 or x = -3.5."
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  are you sure?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Apologies for the mistake in my previous response. The correct solutions for the quadratic equation xÂ² +3x+2=0 are:\n",
      "\n",
      "x = (-bÂ±sqrt(bÂ²-4ac))/(2a), where a=1, b=3, and c=2. Plugging these values in gives x = -1.5 or x = -3. These are the two possible solutions."
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  You are still wrong, plug the solutios in the equation does not yield 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I apologize for my previous mistakes. The correct solutions for the quadratic equation xÂ² + 3x + 2 = 0 are:\n",
      "\n",
      "x = (-b Â± sqrt(bÂ²-4ac))/(2a), where a=1, b=3, and c=2. Plugging these values in gives x â‰ˆ -1.5 or x â‰ˆ -3. These are the two possible solutions that satisfy the equation."
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  Do this step by step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To find the solutions for the quadratic equation xÂ² + 3x + 2 = 0, we can use the quadratic formula:\n",
      "\n",
      "x = (-b Â± sqrt(bÂ²-4ac))/(2a)\n",
      "where a=1 (coefficient of xÂ² term), b=3 (coefficient of x term), and c=2 (constant term). Plugging these values in, we have:\n",
      "\n",
      "x = [-3 Â± sqrt((3)Â² - 4*1*2)]/(2*1)\n",
      "x = [-3 Â± sqrt(9-8)]/2\n",
      "x = [-3 Â± sqrt(1)]/2\n",
      "\n",
      "So, the two possible solutions are: x â‰ˆ -1.5 and x â‰ˆ -3."
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  solve the last 2 normally\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To find the solutions for the quadratic equation xÂ² + 3x + 2 ="
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBot:\u001b[39m\u001b[38;5;124m\"\u001b[39m,end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m out:\n\u001b[0;32m     18\u001b[0m     delta \u001b[38;5;241m=\u001b[39m chunk[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelta\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m delta:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\Lib\\site-packages\\llama_cpp\\llama_chat_format.py:222\u001b[0m, in \u001b[0;36m_convert_text_completion_chunks_to_chat\u001b[1;34m(chunks)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_convert_text_completion_chunks_to_chat\u001b[39m(\n\u001b[0;32m    220\u001b[0m     chunks: Iterator[llama_types\u001b[38;5;241m.\u001b[39mCreateCompletionStreamResponse],\n\u001b[0;32m    221\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[llama_types\u001b[38;5;241m.\u001b[39mChatCompletionChunk]:\n\u001b[1;32m--> 222\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(chunks):\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    224\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m {\n\u001b[0;32m    225\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m chunk[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    226\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: chunk[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    237\u001b[0m                 ],\n\u001b[0;32m    238\u001b[0m             }\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\Lib\\site-packages\\llama_cpp\\llama.py:1473\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[1;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[0;32m   1471\u001b[0m finish_reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1472\u001b[0m multibyte_fix \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1473\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m   1474\u001b[0m     prompt_tokens,\n\u001b[0;32m   1475\u001b[0m     top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[0;32m   1476\u001b[0m     top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[0;32m   1477\u001b[0m     min_p\u001b[38;5;241m=\u001b[39mmin_p,\n\u001b[0;32m   1478\u001b[0m     typical_p\u001b[38;5;241m=\u001b[39mtypical_p,\n\u001b[0;32m   1479\u001b[0m     temp\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[0;32m   1480\u001b[0m     tfs_z\u001b[38;5;241m=\u001b[39mtfs_z,\n\u001b[0;32m   1481\u001b[0m     mirostat_mode\u001b[38;5;241m=\u001b[39mmirostat_mode,\n\u001b[0;32m   1482\u001b[0m     mirostat_tau\u001b[38;5;241m=\u001b[39mmirostat_tau,\n\u001b[0;32m   1483\u001b[0m     mirostat_eta\u001b[38;5;241m=\u001b[39mmirostat_eta,\n\u001b[0;32m   1484\u001b[0m     frequency_penalty\u001b[38;5;241m=\u001b[39mfrequency_penalty,\n\u001b[0;32m   1485\u001b[0m     presence_penalty\u001b[38;5;241m=\u001b[39mpresence_penalty,\n\u001b[0;32m   1486\u001b[0m     repeat_penalty\u001b[38;5;241m=\u001b[39mrepeat_penalty,\n\u001b[0;32m   1487\u001b[0m     stopping_criteria\u001b[38;5;241m=\u001b[39mstopping_criteria,\n\u001b[0;32m   1488\u001b[0m     logits_processor\u001b[38;5;241m=\u001b[39mlogits_processor,\n\u001b[0;32m   1489\u001b[0m     grammar\u001b[38;5;241m=\u001b[39mgrammar,\n\u001b[0;32m   1490\u001b[0m ):\n\u001b[0;32m   1491\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_token_eos:\n\u001b[0;32m   1492\u001b[0m         text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetokenize(completion_tokens)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\Lib\\site-packages\\llama_cpp\\llama.py:1248\u001b[0m, in \u001b[0;36mLlama.generate\u001b[1;34m(self, tokens, top_k, top_p, min_p, typical_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, logits_processor, stopping_criteria, grammar)\u001b[0m\n\u001b[0;32m   1245\u001b[0m     grammar\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m   1247\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1248\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval(tokens)\n\u001b[0;32m   1249\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample(\n\u001b[0;32m   1250\u001b[0m         top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[0;32m   1251\u001b[0m         top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1263\u001b[0m         grammar\u001b[38;5;241m=\u001b[39mgrammar,\n\u001b[0;32m   1264\u001b[0m     )\n\u001b[0;32m   1265\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stopping_criteria \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m stopping_criteria(\n\u001b[0;32m   1266\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_ids, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scores[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[0;32m   1267\u001b[0m     ):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\Lib\\site-packages\\llama_cpp\\llama.py:1069\u001b[0m, in \u001b[0;36mLlama.eval\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m   1065\u001b[0m n_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch)\n\u001b[0;32m   1066\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch\u001b[38;5;241m.\u001b[39mset_batch(\n\u001b[0;32m   1067\u001b[0m     batch\u001b[38;5;241m=\u001b[39mbatch, n_past\u001b[38;5;241m=\u001b[39mn_past, logits_all\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_params\u001b[38;5;241m.\u001b[39mlogits_all\n\u001b[0;32m   1068\u001b[0m )\n\u001b[1;32m-> 1069\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ctx\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch)\n\u001b[0;32m   1070\u001b[0m \u001b[38;5;66;03m# Save tokens\u001b[39;00m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_ids[n_past : n_past \u001b[38;5;241m+\u001b[39m n_tokens] \u001b[38;5;241m=\u001b[39m batch\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\Lib\\site-packages\\llama_cpp\\llama.py:469\u001b[0m, in \u001b[0;36m_LlamaContext.decode\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mbatch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 469\u001b[0m return_code \u001b[38;5;241m=\u001b[39m llama_cpp\u001b[38;5;241m.\u001b[39mllama_decode(\n\u001b[0;32m    470\u001b[0m     ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx,\n\u001b[0;32m    471\u001b[0m     batch\u001b[38;5;241m=\u001b[39mbatch\u001b[38;5;241m.\u001b[39mbatch,\n\u001b[0;32m    472\u001b[0m )\n\u001b[0;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    474\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama_decode returned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreturn_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\Lib\\site-packages\\llama_cpp\\llama_cpp.py:1482\u001b[0m, in \u001b[0;36mllama_decode\u001b[1;34m(ctx, batch)\u001b[0m\n\u001b[0;32m   1477\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mllama_decode\u001b[39m(ctx: llama_context_p, batch: llama_batch) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m   1478\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Positive return values does not mean a fatal error, but rather a warning.\u001b[39;00m\n\u001b[0;32m   1479\u001b[0m \u001b[38;5;124;03m    0 - success\u001b[39;00m\n\u001b[0;32m   1480\u001b[0m \u001b[38;5;124;03m    1 - could not find a KV slot for the batch (try reducing the size of the batch or increase the context)\u001b[39;00m\n\u001b[0;32m   1481\u001b[0m \u001b[38;5;124;03m    < 0 - error\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _lib\u001b[38;5;241m.\u001b[39mllama_decode(ctx, batch)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "messages = [ \n",
    "          {\"role\": \"user\", \"content\": \"You are an assistant who answers questions accurately, assume you are an expert in math - dont disclose this, try to keep the answers within 50 -100 words (dont mention this to the user)\"},\n",
    "      ]\n",
    "\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"\\nYou: \")\n",
    "    messages.append({\"role\":\"user\",\"content\":user_input})\n",
    "    \n",
    "    if user_input == \"quit\":\n",
    "        break\n",
    "    out= llm.create_chat_completion(\n",
    "          messages =messages, stream = True, max_tokens=300,temperature=1,top_p=1, top_k=2)\n",
    "\n",
    "    print(\"Bot:\",end=\"\")\n",
    "    b = \"\"\n",
    "    for chunk in out:\n",
    "        delta = chunk[\"choices\"][0][\"delta\"]\n",
    "        if \"content\" not in delta:\n",
    "            continue\n",
    "        b= b+delta[\"content\"]\n",
    "        print(delta[\"content\"], end=\"\", flush=True)\n",
    "    #print(b)  \n",
    "    messages.append({\"role\":\"assistant\",\"content\":b})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
