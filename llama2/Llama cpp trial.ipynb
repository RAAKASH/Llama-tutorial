{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a43f0874-014e-4eb1-ac19-8f3f50938922",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda config --add channels conda-forge\n",
    "#!conda config --set channel_priority strict\n",
    "#!conda install llama-cpp-python\n",
    "#!conda install -c conda-forge huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423efa60-d26e-41f6-a25a-ae9d2a6dc12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!huggingface-cli download TheBloke/Llama-2-7B-GGUF llama-2-7b.Q8_0.gguf --local-dir . --local-dir-use-symlinks False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c61f412f-20a4-4fdc-b4d5-d58ecac14ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69a28390-9d14-4c4d-826b-7b66a05fb428",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "llm = Llama(\n",
    "      model_path=\"./models/llama-2-7b.Q2_K.gguf\",\n",
    "      chat_format=\"llama-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2da96fd-9001-4541-b4ff-138fb908f45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    }
   ],
   "source": [
    "output = llm(\n",
    "      \"Q: Name the planets in the solar system? A: \", # Prompt\n",
    "      max_tokens=512, # Generate up to 512 tokens, set to None to generate up to the end of the context window\n",
    "      stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
    "      echo=True # Echo the prompt back in the output\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4642408c-b198-4ba0-9bde-675b1505ebb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q: Name the planets in the solar system? A: 9 Planets in solar system. профсiпs, 5 Dwarf Planets'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['choices'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "055bd945-3e42-40f2-9b51-9c3d92f3ff20",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Llama.generate() got an unexpected keyword argument 'max_tokens'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m w \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#for token in llm.generate(tokens, top_k=40, top_p=0.95, temp=1.0, repeat_penalty=1.1):\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#    w = w+ str(llm.detokenize([token]))\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#    print(w)\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m output \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39mgenerate(tokens, top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m, top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m, temp\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, repeat_penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.1\u001b[39m,max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Llama.generate() got an unexpected keyword argument 'max_tokens'"
     ]
    }
   ],
   "source": [
    "tokens = llm.tokenize(b\"Q: Name the planets in the solar system? A:\")\n",
    "w = \"\"\n",
    "#for token in llm.generate(tokens, top_k=40, top_p=0.95, temp=1.0, repeat_penalty=1.1):\n",
    "#    w = w+ str(llm.detokenize([token]))\n",
    "#    print(w)\n",
    "output = llm.generate(tokens, top_k=40, top_p=0.95, temp=1.0, repeat_penalty=1.1,max_tokens=40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ebf1ffc8-ed97-4000-b0a2-849abb2cf0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m llm\u001b[38;5;241m.\u001b[39mdetokenize(output)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\Lib\\site-packages\\llama_cpp\\llama.py:1030\u001b[0m, in \u001b[0;36mLlama.detokenize\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdetokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens: List[\u001b[38;5;28mint\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbytes\u001b[39m:\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Detokenize a list of tokens.\u001b[39;00m\n\u001b[0;32m   1023\u001b[0m \n\u001b[0;32m   1024\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1028\u001b[0m \u001b[38;5;124;03m        The detokenized string.\u001b[39;00m\n\u001b[0;32m   1029\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1030\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mdetokenize(tokens)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\Lib\\site-packages\\llama_cpp\\llama.py:379\u001b[0m, in \u001b[0;36m_LlamaModel.detokenize\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m    377\u001b[0m size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[0;32m    378\u001b[0m buffer \u001b[38;5;241m=\u001b[39m (ctypes\u001b[38;5;241m.\u001b[39mc_char \u001b[38;5;241m*\u001b[39m size)()\n\u001b[1;32m--> 379\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens:\n\u001b[0;32m    380\u001b[0m     n \u001b[38;5;241m=\u001b[39m llama_cpp\u001b[38;5;241m.\u001b[39mllama_token_to_piece(\n\u001b[0;32m    381\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, llama_cpp\u001b[38;5;241m.\u001b[39mllama_token(token), buffer, size\n\u001b[0;32m    382\u001b[0m     )\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m n \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m size\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\Lib\\site-packages\\llama_cpp\\llama.py:1248\u001b[0m, in \u001b[0;36mLlama.generate\u001b[1;34m(self, tokens, top_k, top_p, min_p, typical_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, logits_processor, stopping_criteria, grammar)\u001b[0m\n\u001b[0;32m   1245\u001b[0m     grammar\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m   1247\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1248\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval(tokens)\n\u001b[0;32m   1249\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample(\n\u001b[0;32m   1250\u001b[0m         top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[0;32m   1251\u001b[0m         top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1263\u001b[0m         grammar\u001b[38;5;241m=\u001b[39mgrammar,\n\u001b[0;32m   1264\u001b[0m     )\n\u001b[0;32m   1265\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stopping_criteria \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m stopping_criteria(\n\u001b[0;32m   1266\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_ids, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scores[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[0;32m   1267\u001b[0m     ):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\Lib\\site-packages\\llama_cpp\\llama.py:1069\u001b[0m, in \u001b[0;36mLlama.eval\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m   1065\u001b[0m n_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch)\n\u001b[0;32m   1066\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch\u001b[38;5;241m.\u001b[39mset_batch(\n\u001b[0;32m   1067\u001b[0m     batch\u001b[38;5;241m=\u001b[39mbatch, n_past\u001b[38;5;241m=\u001b[39mn_past, logits_all\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_params\u001b[38;5;241m.\u001b[39mlogits_all\n\u001b[0;32m   1068\u001b[0m )\n\u001b[1;32m-> 1069\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ctx\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch)\n\u001b[0;32m   1070\u001b[0m \u001b[38;5;66;03m# Save tokens\u001b[39;00m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_ids[n_past : n_past \u001b[38;5;241m+\u001b[39m n_tokens] \u001b[38;5;241m=\u001b[39m batch\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\Lib\\site-packages\\llama_cpp\\llama.py:469\u001b[0m, in \u001b[0;36m_LlamaContext.decode\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mbatch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 469\u001b[0m return_code \u001b[38;5;241m=\u001b[39m llama_cpp\u001b[38;5;241m.\u001b[39mllama_decode(\n\u001b[0;32m    470\u001b[0m     ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx,\n\u001b[0;32m    471\u001b[0m     batch\u001b[38;5;241m=\u001b[39mbatch\u001b[38;5;241m.\u001b[39mbatch,\n\u001b[0;32m    472\u001b[0m )\n\u001b[0;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    474\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama_decode returned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreturn_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\Lib\\site-packages\\llama_cpp\\llama_cpp.py:1482\u001b[0m, in \u001b[0;36mllama_decode\u001b[1;34m(ctx, batch)\u001b[0m\n\u001b[0;32m   1477\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mllama_decode\u001b[39m(ctx: llama_context_p, batch: llama_batch) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m   1478\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Positive return values does not mean a fatal error, but rather a warning.\u001b[39;00m\n\u001b[0;32m   1479\u001b[0m \u001b[38;5;124;03m    0 - success\u001b[39;00m\n\u001b[0;32m   1480\u001b[0m \u001b[38;5;124;03m    1 - could not find a KV slot for the batch (try reducing the size of the batch or increase the context)\u001b[39;00m\n\u001b[0;32m   1481\u001b[0m \u001b[38;5;124;03m    < 0 - error\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _lib\u001b[38;5;241m.\u001b[39mllama_decode(ctx, batch)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "llm.detokenize(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "363d7fc4-4928-4cd2-995f-69b3229b7c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "llm2 = Llama(\n",
    "      model_path=\"./models/llama-2-7b.Q8_0.gguf\",\n",
    "      chat_format=\"llama-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7d6398d7-b93c-47dc-a49f-5189e0234607",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm2(\n",
    "      \"Q: Name the planets in the solar system? A: \", # Prompt\n",
    "      max_tokens=32, # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
    "      stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
    "      echo=True # Echo the prompt back in the output\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6b0d723a-6b1b-4def-8236-d842129d48f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-70095ee8-a6a9-40b3-a197-7d42b801f27f', 'object': 'text_completion', 'created': 1709228309, 'model': './models/llama-2-7b.Q8_0.gguf', 'choices': [{'text': 'Q: Name the planets in the solar system? A: 9 and a half.', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 15, 'completion_tokens': 6, 'total_tokens': 21}}\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9e384e15-70f8-44b5-8d59-daa726548f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-031acb64-ab75-429d-b86a-d70e1b2be2dd',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1709228553,\n",
       " 'model': './models/llama-2-7b.Q2_K.gguf',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': \"\\nI'm not sure what to do with this. I've never seen anything like it before.\\nIt looks like a picture of a man, but he doesn't look human at all. He has no eyes and his mouth is open as if screaming. His skin is pale white and he seems to be wearing some sort of armor.\\nI can't make out what the writing says on it though...\\nIt looks like a picture of a man, but he doesn't look human at all. He has no eyes and his mouth is open as if screaming. His skin is pale white and he seems to be wearing some sort of armor. I can't make out what the writing says on it though...\\nI think that this picture is a representation of the human condition. The man in the photo has no eyes, which means he cannot see anything around him. He also appears to be screaming as if in pain or fear, which could indicate some sort of mental illness or trauma from his past experiences with others who may have hurt him physically or emotionally (or both). His skin is pale white because it's been exposed to too much sunlight over time without protection; this could also be due to poor diet choices leading up until now when he finally got sick enough that doctors had no choice but treat him medically instead of just giving him vitamins like they normally would have done beforehand (which would have helped prevent any further damage).\\nThe writing on the photo says something about how people should be more careful with their health because it's easy for them to get sick if they don't take care of themselves properly first off by eating right and exercising regularly enough so that their bodies can function at optimal levels all day long instead relying solely upon medication when things go wrong later down the road after years have passed since then originally started out doing everything correctly beforehand anyway already without any problems whatsoever occurring during those times either way though nowadays though now though now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now now\"},\n",
       "   'finish_reason': 'length'}],\n",
       " 'usage': {'prompt_tokens': 37, 'completion_tokens': 475, 'total_tokens': 512}}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm2.create_chat_completion(\n",
    "      messages = [\n",
    "          {\"role\": \"system\", \"content\": \"You are an assistant who perfectly describes images.\"},\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": \"Describe this image in detail please.\"\n",
    "          }\n",
    "      ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ea2ad46d-c22e-491e-bb9a-1a59b1b97118",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m llm2\u001b[38;5;241m.\u001b[39mcreate_chat_completion(\n\u001b[0;32m      2\u001b[0m       messages \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      3\u001b[0m           {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are an assistant who answers accurately.\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m      4\u001b[0m           {\n\u001b[0;32m      5\u001b[0m               \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m               \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExplain the fission process in 20 words.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m           }\n\u001b[0;32m      8\u001b[0m       ]\n\u001b[0;32m      9\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\Lib\\site-packages\\llama_cpp\\llama.py:2101\u001b[0m, in \u001b[0;36mLlama.create_chat_completion\u001b[1;34m(self, messages, functions, function_call, tools, tool_choice, temperature, top_p, top_k, min_p, typical_p, stream, stop, seed, response_format, max_tokens, presence_penalty, frequency_penalty, repeat_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[0;32m   2065\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate a chat completion from a list of messages.\u001b[39;00m\n\u001b[0;32m   2066\u001b[0m \n\u001b[0;32m   2067\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2096\u001b[0m \u001b[38;5;124;03m    Generated chat completion or a stream of chat completion chunks.\u001b[39;00m\n\u001b[0;32m   2097\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2098\u001b[0m handler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_handler \u001b[38;5;129;01mor\u001b[39;00m llama_chat_format\u001b[38;5;241m.\u001b[39mget_chat_completion_handler(\n\u001b[0;32m   2099\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_format\n\u001b[0;32m   2100\u001b[0m )\n\u001b[1;32m-> 2101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m handler(\n\u001b[0;32m   2102\u001b[0m     llama\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2103\u001b[0m     messages\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[0;32m   2104\u001b[0m     functions\u001b[38;5;241m=\u001b[39mfunctions,\n\u001b[0;32m   2105\u001b[0m     function_call\u001b[38;5;241m=\u001b[39mfunction_call,\n\u001b[0;32m   2106\u001b[0m     tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[0;32m   2107\u001b[0m     tool_choice\u001b[38;5;241m=\u001b[39mtool_choice,\n\u001b[0;32m   2108\u001b[0m     temperature\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[0;32m   2109\u001b[0m     top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[0;32m   2110\u001b[0m     top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[0;32m   2111\u001b[0m     min_p\u001b[38;5;241m=\u001b[39mmin_p,\n\u001b[0;32m   2112\u001b[0m     typical_p\u001b[38;5;241m=\u001b[39mtypical_p,\n\u001b[0;32m   2113\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   2114\u001b[0m     stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m   2115\u001b[0m     seed\u001b[38;5;241m=\u001b[39mseed,\n\u001b[0;32m   2116\u001b[0m     response_format\u001b[38;5;241m=\u001b[39mresponse_format,\n\u001b[0;32m   2117\u001b[0m     max_tokens\u001b[38;5;241m=\u001b[39mmax_tokens,\n\u001b[0;32m   2118\u001b[0m     presence_penalty\u001b[38;5;241m=\u001b[39mpresence_penalty,\n\u001b[0;32m   2119\u001b[0m     frequency_penalty\u001b[38;5;241m=\u001b[39mfrequency_penalty,\n\u001b[0;32m   2120\u001b[0m     repeat_penalty\u001b[38;5;241m=\u001b[39mrepeat_penalty,\n\u001b[0;32m   2121\u001b[0m     tfs_z\u001b[38;5;241m=\u001b[39mtfs_z,\n\u001b[0;32m   2122\u001b[0m     mirostat_mode\u001b[38;5;241m=\u001b[39mmirostat_mode,\n\u001b[0;32m   2123\u001b[0m     mirostat_tau\u001b[38;5;241m=\u001b[39mmirostat_tau,\n\u001b[0;32m   2124\u001b[0m     mirostat_eta\u001b[38;5;241m=\u001b[39mmirostat_eta,\n\u001b[0;32m   2125\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   2126\u001b[0m     logits_processor\u001b[38;5;241m=\u001b[39mlogits_processor,\n\u001b[0;32m   2127\u001b[0m     grammar\u001b[38;5;241m=\u001b[39mgrammar,\n\u001b[0;32m   2128\u001b[0m     logit_bias\u001b[38;5;241m=\u001b[39mlogit_bias,\n\u001b[0;32m   2129\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\Lib\\site-packages\\llama_cpp\\llama_chat_format.py:334\u001b[0m, in \u001b[0;36mregister_chat_format.<locals>.decorator.<locals>.basic_create_chat_completion\u001b[1;34m(llama, messages, functions, function_call, tools, tool_choice, temperature, top_p, top_k, min_p, typical_p, stream, stop, seed, response_format, max_tokens, presence_penalty, frequency_penalty, repeat_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, logits_processor, grammar, logit_bias, **kwargs)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m response_format[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson_object\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    330\u001b[0m     grammar \u001b[38;5;241m=\u001b[39m llama_grammar\u001b[38;5;241m.\u001b[39mLlamaGrammar\u001b[38;5;241m.\u001b[39mfrom_string(\n\u001b[0;32m    331\u001b[0m         llama_grammar\u001b[38;5;241m.\u001b[39mJSON_GBNF\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 334\u001b[0m completion_or_chunks \u001b[38;5;241m=\u001b[39m llama\u001b[38;5;241m.\u001b[39mcreate_completion(\n\u001b[0;32m    335\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[0;32m    336\u001b[0m     temperature\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[0;32m    337\u001b[0m     top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[0;32m    338\u001b[0m     top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[0;32m    339\u001b[0m     min_p\u001b[38;5;241m=\u001b[39mmin_p,\n\u001b[0;32m    340\u001b[0m     typical_p\u001b[38;5;241m=\u001b[39mtypical_p,\n\u001b[0;32m    341\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    342\u001b[0m     stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    343\u001b[0m     seed\u001b[38;5;241m=\u001b[39mseed,\n\u001b[0;32m    344\u001b[0m     max_tokens\u001b[38;5;241m=\u001b[39mmax_tokens,\n\u001b[0;32m    345\u001b[0m     presence_penalty\u001b[38;5;241m=\u001b[39mpresence_penalty,\n\u001b[0;32m    346\u001b[0m     frequency_penalty\u001b[38;5;241m=\u001b[39mfrequency_penalty,\n\u001b[0;32m    347\u001b[0m     repeat_penalty\u001b[38;5;241m=\u001b[39mrepeat_penalty,\n\u001b[0;32m    348\u001b[0m     tfs_z\u001b[38;5;241m=\u001b[39mtfs_z,\n\u001b[0;32m    349\u001b[0m     mirostat_mode\u001b[38;5;241m=\u001b[39mmirostat_mode,\n\u001b[0;32m    350\u001b[0m     mirostat_tau\u001b[38;5;241m=\u001b[39mmirostat_tau,\n\u001b[0;32m    351\u001b[0m     mirostat_eta\u001b[38;5;241m=\u001b[39mmirostat_eta,\n\u001b[0;32m    352\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    353\u001b[0m     logits_processor\u001b[38;5;241m=\u001b[39mlogits_processor,\n\u001b[0;32m    354\u001b[0m     grammar\u001b[38;5;241m=\u001b[39mgrammar,\n\u001b[0;32m    355\u001b[0m     logit_bias\u001b[38;5;241m=\u001b[39mlogit_bias,\n\u001b[0;32m    356\u001b[0m )\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _convert_completion_to_chat(completion_or_chunks, stream\u001b[38;5;241m=\u001b[39mstream)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\Lib\\site-packages\\llama_cpp\\llama.py:1939\u001b[0m, in \u001b[0;36mLlama.create_completion\u001b[1;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[0;32m   1937\u001b[0m     chunks: Iterator[CreateCompletionStreamResponse] \u001b[38;5;241m=\u001b[39m completion_or_chunks\n\u001b[0;32m   1938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m chunks\n\u001b[1;32m-> 1939\u001b[0m completion: Completion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(completion_or_chunks)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   1940\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m completion\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\Lib\\site-packages\\llama_cpp\\llama.py:1473\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[1;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[0;32m   1471\u001b[0m finish_reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1472\u001b[0m multibyte_fix \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1473\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m   1474\u001b[0m     prompt_tokens,\n\u001b[0;32m   1475\u001b[0m     top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[0;32m   1476\u001b[0m     top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[0;32m   1477\u001b[0m     min_p\u001b[38;5;241m=\u001b[39mmin_p,\n\u001b[0;32m   1478\u001b[0m     typical_p\u001b[38;5;241m=\u001b[39mtypical_p,\n\u001b[0;32m   1479\u001b[0m     temp\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[0;32m   1480\u001b[0m     tfs_z\u001b[38;5;241m=\u001b[39mtfs_z,\n\u001b[0;32m   1481\u001b[0m     mirostat_mode\u001b[38;5;241m=\u001b[39mmirostat_mode,\n\u001b[0;32m   1482\u001b[0m     mirostat_tau\u001b[38;5;241m=\u001b[39mmirostat_tau,\n\u001b[0;32m   1483\u001b[0m     mirostat_eta\u001b[38;5;241m=\u001b[39mmirostat_eta,\n\u001b[0;32m   1484\u001b[0m     frequency_penalty\u001b[38;5;241m=\u001b[39mfrequency_penalty,\n\u001b[0;32m   1485\u001b[0m     presence_penalty\u001b[38;5;241m=\u001b[39mpresence_penalty,\n\u001b[0;32m   1486\u001b[0m     repeat_penalty\u001b[38;5;241m=\u001b[39mrepeat_penalty,\n\u001b[0;32m   1487\u001b[0m     stopping_criteria\u001b[38;5;241m=\u001b[39mstopping_criteria,\n\u001b[0;32m   1488\u001b[0m     logits_processor\u001b[38;5;241m=\u001b[39mlogits_processor,\n\u001b[0;32m   1489\u001b[0m     grammar\u001b[38;5;241m=\u001b[39mgrammar,\n\u001b[0;32m   1490\u001b[0m ):\n\u001b[0;32m   1491\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_token_eos:\n\u001b[0;32m   1492\u001b[0m         text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetokenize(completion_tokens)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\Lib\\site-packages\\llama_cpp\\llama.py:1248\u001b[0m, in \u001b[0;36mLlama.generate\u001b[1;34m(self, tokens, top_k, top_p, min_p, typical_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, logits_processor, stopping_criteria, grammar)\u001b[0m\n\u001b[0;32m   1245\u001b[0m     grammar\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m   1247\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1248\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval(tokens)\n\u001b[0;32m   1249\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample(\n\u001b[0;32m   1250\u001b[0m         top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[0;32m   1251\u001b[0m         top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1263\u001b[0m         grammar\u001b[38;5;241m=\u001b[39mgrammar,\n\u001b[0;32m   1264\u001b[0m     )\n\u001b[0;32m   1265\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stopping_criteria \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m stopping_criteria(\n\u001b[0;32m   1266\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_ids, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scores[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[0;32m   1267\u001b[0m     ):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\Lib\\site-packages\\llama_cpp\\llama.py:1069\u001b[0m, in \u001b[0;36mLlama.eval\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m   1065\u001b[0m n_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch)\n\u001b[0;32m   1066\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch\u001b[38;5;241m.\u001b[39mset_batch(\n\u001b[0;32m   1067\u001b[0m     batch\u001b[38;5;241m=\u001b[39mbatch, n_past\u001b[38;5;241m=\u001b[39mn_past, logits_all\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_params\u001b[38;5;241m.\u001b[39mlogits_all\n\u001b[0;32m   1068\u001b[0m )\n\u001b[1;32m-> 1069\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ctx\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch)\n\u001b[0;32m   1070\u001b[0m \u001b[38;5;66;03m# Save tokens\u001b[39;00m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_ids[n_past : n_past \u001b[38;5;241m+\u001b[39m n_tokens] \u001b[38;5;241m=\u001b[39m batch\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\Lib\\site-packages\\llama_cpp\\llama.py:469\u001b[0m, in \u001b[0;36m_LlamaContext.decode\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mbatch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 469\u001b[0m return_code \u001b[38;5;241m=\u001b[39m llama_cpp\u001b[38;5;241m.\u001b[39mllama_decode(\n\u001b[0;32m    470\u001b[0m     ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx,\n\u001b[0;32m    471\u001b[0m     batch\u001b[38;5;241m=\u001b[39mbatch\u001b[38;5;241m.\u001b[39mbatch,\n\u001b[0;32m    472\u001b[0m )\n\u001b[0;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    474\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama_decode returned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreturn_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\Lib\\site-packages\\llama_cpp\\llama_cpp.py:1482\u001b[0m, in \u001b[0;36mllama_decode\u001b[1;34m(ctx, batch)\u001b[0m\n\u001b[0;32m   1477\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mllama_decode\u001b[39m(ctx: llama_context_p, batch: llama_batch) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m   1478\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Positive return values does not mean a fatal error, but rather a warning.\u001b[39;00m\n\u001b[0;32m   1479\u001b[0m \u001b[38;5;124;03m    0 - success\u001b[39;00m\n\u001b[0;32m   1480\u001b[0m \u001b[38;5;124;03m    1 - could not find a KV slot for the batch (try reducing the size of the batch or increase the context)\u001b[39;00m\n\u001b[0;32m   1481\u001b[0m \u001b[38;5;124;03m    < 0 - error\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _lib\u001b[38;5;241m.\u001b[39mllama_decode(ctx, batch)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "llm2.create_chat_completion(\n",
    "      messages = [\n",
    "          {\"role\": \"system\", \"content\": \"You are an assistant who answers accurately.\"},\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": \"Explain the fission process in 20 words.\"\n",
    "          }\n",
    "      ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fe52e0b1-f5c1-481a-8c9e-c59779064e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-0d3aed35-27ee-4254-9291-5667cfbf916b',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1709231118,\n",
       " 'model': './models/llama-2-7b.Q2_K.gguf',\n",
       " 'choices': [{'text': 'Q: Explain the fission process in detail? A: 1. Einzelnuklid löst sich im Zuge von Protonen-Antiprotonen-Kollisionen in zwei weitere nukleare Teilchen, die so genannte „nuklearen Kerne“, aus. Die Teilchen verlieren dabei Masse und Energie an ihr Umfeld. Diese Kernen kollidieren nach und nach in der Kernzone mit den daran grenzenden Kernen und lösen diese wiederum ab. Das erklärt die kontinuierliche Radioaktivität der Atombomben. 2. Der Neutronenabgabeprozess: Ein zerfällender Nukleide verliert durch die Zusammenschlagung mit einem Wasserstoffkern (Proton) einen Teilchen und löst so ein Kern ab, welches dann in der Zerfallsphase den Prozessen der nuklearen Radioaktivität di',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'length'}],\n",
       " 'usage': {'prompt_tokens': 15, 'completion_tokens': 200, 'total_tokens': 215}}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\n",
    "      \"Q: Explain the fission process in detail? A: \", # Prompt\n",
    "      max_tokens=200, # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
    "      stop=[\"Q:\"], # Stop generating just before the model would generate a new question\n",
    "      echo=True # Echo the prompt back in the output\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef59378-4a3e-4e4a-9220-67c8ebe44655",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    }
   ],
   "source": [
    "llm2.create_chat_completion(\n",
    "      messages = [\n",
    "          {\"role\": \"system\", \"content\": \"You are an assistant who answers accurately.\"},\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": \"Explain the fission process in detail\"\n",
    "          }\n",
    "      ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c94f7c5b-24e9-4057-80b3-1f799a2f4b85",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'Llama' has no attribute 'from_pretrained'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_cpp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Llama\n\u001b[1;32m----> 2\u001b[0m llm \u001b[38;5;241m=\u001b[39m Llama\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m      3\u001b[0m     repo_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen/Qwen1.5-0.5B-Chat-GGUF\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      4\u001b[0m     filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*q8_0.gguf\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m      6\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'Llama' has no attribute 'from_pretrained'"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "llm = Llama.from_pretrained(\n",
    "    repo_id=\"Qwen/Qwen1.5-0.5B-Chat-GGUF\",\n",
    "    filename=\"*q8_0.gguf\",\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d978852-1dbc-4ccf-9e70-4e8cf4977d9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
